\documentclass{article}

\usepackage{amsmath,amssymb, amsfonts}

\begin{document}
\section{Piman-Yor Diffusion Trees}
\subsection{Modelling of PYDT}	

	\begin{eqnarray}
		\alpha &\sim& \textrm{Beta}(\alpha | a_\alpha, b_\alpha) \\ 
		\theta &\sim& \textrm{G}(\theta | a_\theta, b_\theta) \\
		p(\mathcal{T}|\alpha, \theta) &=& \prod_{v \in \mathcal{I}} \frac{a(t_v) \prod^{K_v}_{k=3}[\theta + (k - 1)\alpha] \prod^{K_v}_{l=1} \Gamma(n^v_l - \alpha)}{\Gamma(m(v) + \theta) \Gamma(1 - \alpha)^{K_v - 1}} \nonumber \\
		\\
		c &\sim& \textrm{G}(c | a_c, b_c) \\
		1/\sigma^2 &\sim& \textrm{G}(1/\sigma^2 | a_{\sigma^2}, b_{\sigma^2}) \\
		p(t_v | c, \mathcal{T}) &=& c(1-t_v)^{cJ^{\theta,\alpha}_{\bold{n}_v} - 1} \nonumber \\
		&=& c\exp \big\{ (cJ^{\theta,\alpha}_{\bold{n}_v} - 1)\ln{(1-t_v)} \big\} \nonumber \\
		&=& c\exp \big\{ cJ^{\theta,\alpha}_{\bold{n}_v}\ln{(1-t_v)} \big\} \exp \big\{-\ln{(1-t_v)} \big\} \\
		\mathcal{N}(\bold{z}_v | \bold{z}_u, \sigma^2(t_v - t_u)\bold{I}) &=& (2 \pi \sigma^2(t_v - t_u))^{-\frac{D}{2}} \exp \bigg\{-\frac{1}{2}\frac{\parallel \bold{z}_v - \bold{z}_u \parallel^2}{\sigma^2 (t_v - t_u)} \bigg\} \nonumber \\
		\\
		\mathcal{N}(\bold{z}_k | \bold{z}_v, \sigma^2(t_k - t_v)\bold{I}) &=& (2 \pi \sigma^2(t_k - t_v))^{-\frac{D}{2}} \exp \bigg\{-\frac{1}{2}\frac{\parallel \bold{z}_k - \bold{z}_v \parallel^2}{\sigma^2 (t_k - t_v)} \bigg\} \nonumber \\
		\\
		\mathcal{N}(\bold{x}_n | \bold{z}_{\text{pa}(n)}, \sigma^2(1 - t_{\text{pa}(n)})\bold{I}) &=& (2 \pi \sigma^2(1 - t_{\text{pa}(n)}))^{-\frac{D}{2}} \exp \bigg\{-\frac{1}{2}\frac{\parallel \bold{x}_n - \bold{z}_{\text{pa}(n)} \parallel^2}{\sigma^2 (1 - t_{\text{pa}(n)})} \bigg\} \nonumber \\
	\end{eqnarray}
	
	\subsection{Greedy inference algorithm for the PYDT}
	The algorithm consists of three steps. The first step samples the latent variables $c$, $1/\sigma^2$, $\bold{t}$, $\bold{Z}$ from the almost exact posteriors with the fixed tree structure $\mathcal{T}$, $\alpha$ and $\theta$. Since each latent variable is dependent on others, sampling should be cycled a few times as like Gibbs sampling to obtain latent variables which represents the posterior distributions. The second step searches tree structure by detaching a branch randomly and reattaching it to other branching points. A branching point of the reattachment is determined proportional to the model evidence of the new tree structures $p(\bold{X} | \mathcal{T}, \alpha, \theta)$ generated by possible reattachment patterns. Since we have the almost exact posteriors of the latent variables, the model evidence can be derived by $p(x | \theta) = \frac{p(x, z | \theta)}{p(z|x, \theta)}$. This second step can be considered as a sampling of tree structures. Finally, the model parameters $\alpha$ and $\theta$ are updated by maximising the model evidence $p(\bold{X} | \mathcal{T}, \alpha, \theta)$ given by the current tree structure $\mathcal{T}$ obtained at the second step. The idea of the optimisation taken in this third step is in the same spirit of the EM algorithm and variational inference.
	
	\subsection{Posterior of $c$ and $1/\sigma^2$}
	\begin{eqnarray}
		c &\sim& \textrm{G} \bigg( c \ \bigg| \ a_c + |\mathcal{I}|, b_c - \sum_{v \in \mathcal{I}} J^{\theta, \alpha}_{\bold{n}_v} \ln{(1 - t_v)} \bigg) \\
		1/\sigma^2 &\sim& \textrm{G} \Bigg( 1/\sigma^2 \ \Bigg| \ a_{\sigma^2} + \frac{D}{2} \big( |\mathcal{I}| + N \big), b_{\sigma^2} + \frac{1}{2}\sum_{[uv] \in S(\mathcal{T})} \frac{\parallel \bold{z}_v - \bold{z}_u \parallel^2}{t_v - t_u} \bigg) \Bigg) \nonumber \\
	\end{eqnarray}

	
	\subsection{Posterior of $p(\bold{z}_v | \bold{z}_{\{u, k\}}, t_{\{u, v, k\}}, \sigma^2, \mathcal{T})$}	

	The posterior distribution of $\bold{z}$ can be obtained by formulating the below joint distribution as a normal distribution, thanks to the conjugacy among the distributions because of that the prior of mean is modelled as a normal distribution.

	\begin{eqnarray}
	& & p(\bold{z}_v | \bold{z}_{\{u, k\}}, t_{\{u, v, k\}}, \sigma^2, \mathcal{T}) \nonumber \\
	&\propto &
	\mathcal{N}(\bold{z}_v | \bold{z}_u, \sigma^2(t_v - t_u)\bold{I})
	\prod^{K}_{k=1} \mathcal{N}(\bold{z}_k | \bold{z}_v, \sigma^2(t_k - t_v)\bold{I}) \nonumber \\
	&=& \mathcal{N} \Bigg( \bold{z}_v \Bigg| r \bigg( \frac{\bold{z}_u}{t_v - t_u} + \sum_k \frac{\bold{z}_k}{t_k - t_v} \bigg), r\sigma^2\bold{I} \Bigg) \nonumber \\
	& & \textrm{where } r = \bigg( \frac{1}{t_v - t_u} + \sum_k \frac{1}{t_k - t_v} \bigg)^{-1} \nonumber
	\end{eqnarray}

	\subsection{Posterior of $p(t_v | \mathcal{T})$}	
	First, numerically calculate $Z$ and then use this with the joint distribution to numerically approximate the posterior.
	
	\begin{eqnarray}
	p(t_v, \bold{z}_{\{u,v,k\}}, \sigma^2 | c, \mathcal{T}) & = & c(2 \pi \sigma^2)^{-\frac{D(K + 1)}{2}}\exp \Big\{(cJ^{\theta,\alpha}_{\bold{n}_v} - 1)\ln(1 - t_v) \nonumber \\
	 & & - \frac{D}{2} \big( \ln(t_v - t_u) + \sum_k \ln(t_k - t_v) \big) \nonumber \\
	 & & - \frac{\parallel \bold{z}_v - \bold{z}_u\parallel^2}{2\sigma^2}\frac{1}{t_v - t_u} - \sum_k \frac{\parallel \bold{z}_k - \bold{z}_v\parallel^2}{2\sigma^2}\frac{1}{t_k - t_v} \Big\}  \nonumber\\
	 &=& C_{\sigma^2} \exp \big\{ u(t_v) \big\}\\
	\frac{du}{dt_v} &=& -\frac{cJ^{\theta,\alpha}_{\bold{n}_v} - 1}{1 - t_v} - \frac{D}{2}\big( \frac{1}{t_v - t_u} - \sum_k\frac{1}{t_k - t_v}\big) \nonumber \\
	 & & + \frac{\parallel \bold{z}_v - \bold{z}_u\parallel^2}{2\sigma^2}\frac{1}{(t_v - t_u)^2} - \sum_k \frac{\parallel \bold{z}_k - \bold{z}_v\parallel^2}{2\sigma^2}\frac{1}{(t_k - t_v)^2} \nonumber \\
	 &=& -\frac{cJ^{\theta,\alpha}_{\bold{n}_v} - 1}{1 - t_v} - \frac{1}{2(t_v - t_u)^2} \big(D(t_v - t_u) - \frac{\parallel \bold{z}_v - \bold{z}_u\parallel^2}{\sigma^2} \big) \nonumber \\
	 & & + \sum_k \frac{1}{2(t_k - t_v)^2} \big(D(t_k - t_v) - \frac{\parallel \bold{z}_k - \bold{z}_v\parallel^2}{\sigma^2} \big) \nonumber \\
	 &=& A(t_v)\\
	 C_{\sigma^2} A(t_v)^{-1} \int \exp \big\{u(t_v)\big\}du &=& Z, \textrm{ but this integral is intractable!} \\
	 &\Rightarrow & \textrm{The range of integration w.r.t. $t_v$ is $0 < t_u \leq t_v \leq \min(t_k) < 1$, } \nonumber \\
	 & & \textrm{obtaining $Z$ by simply summing pdf by the interval $dt = 1e^{-2}$} \nonumber \\
	 & & \textrm{is computationally tractable for modern hardwares.} \nonumber \\
	 & & \textrm{A few $t$ points usually give sufficient accuracy.} \nonumber \\
	\mathbb{E}_{p(t_v | \bold{z}_{\{u,v,k\}}, \sigma^2, c, \mathcal{T})}[t_v] &=& \frac{1}{Z} \int t_v p(t_v, \bold{z}_{\{u,v,k\}}, \sigma^2 | c, \mathcal{T}) dt_v \nonumber \\
	&=& \frac{1}{Z} C_{\sigma^2} A_\mu(t_v)^{-1} \int \exp \big\{ u_\mu(t_v)\big\}du
	\end{eqnarray}
	
	\subsection{EM algorithm for PYDT}
	\begin{eqnarray}
		\ln P(\bold{X}|\boldsymbol{\theta}) &=& \ln \Big\{\sum_\bold{Z}P(\bold{X}, \bold{Z} | \boldsymbol{\theta}) \Big\} \nonumber \\
		&=& \ln \Big\{\sum_\bold{Z}q(\bold{Z})\frac{P(\bold{X}, \bold{Z}|\boldsymbol{\theta})}{q(\bold{Z})} \Big\} \nonumber \\
		&=& \sum_\bold{Z}q(\bold{Z})\ln \Big\{ \frac{P(\bold{X}, \bold{Z}| \boldsymbol{\theta})}{q(\bold{Z})} \Big\} + \sum_\bold{Z}q(\bold{Z})\ln \Big\{ \frac{q(\bold{Z})}{P(\bold{Z}|\bold{X}, \boldsymbol{\theta})} \Big\}
	\end{eqnarray}

The main procedure of the algorithm is as follows. \\
1) Find $q(\bold{Z})$ which minimizes the KL divergence. \\
2) Take a gradient of the ELBO w.r.t. $\boldsymbol{\theta}$ using $q(\bold{Z})$ found in the step 1).\par
The step 2) can be written as below.

	\begin{eqnarray}
	Q(\boldsymbol{\theta}, \boldsymbol{\theta}') &=& \sum_\bold{Z}q(\bold{Z})\ln \Big\{ \frac{P(\bold{X}, \bold{Z}| \boldsymbol{\theta})}{q(\bold{Z})} \Big\} \nonumber \\
	&=& \sum_\bold{Z}q(\bold{Z}) \ln P(\bold{X}, \bold{Z}| \boldsymbol{\theta}) - \sum_\bold{Z}q(\bold{Z})\ln q(\bold{Z})\\
	\frac{\partial Q(\boldsymbol{\theta}, \boldsymbol{\theta}')}{\partial \boldsymbol{\theta}} &=& \sum_\bold{Z} q(\bold{Z}) \frac{\partial \ln P(\bold{X}, \bold{Z}|\boldsymbol{\theta}) }{\partial \boldsymbol{\theta}} \nonumber \\
	&=& \sum_\bold{Z}p(\bold{Z}|\bold{X}, \boldsymbol{\theta}') \frac{\partial \ln P(\bold{X}, \bold{Z}|\boldsymbol{\theta}) }{\partial \boldsymbol{\theta}}
	\end{eqnarray}
	
	An important point is that the EM algorithm is an algorithm designed for maximum likelihood estimation. Hence, if priors of parameters are combined in the model, EM algorithm becomes the maximum-a-posteriori EM algorithm (MAP-EM) (Gupta and Chen, 2011). Here is an example (Chen and John, 2010).
	
	\begin{eqnarray}
		\mu_j &=& \mu + (j-1)\Delta\mu, j = 1,...,k \\
		\sigma^2_j &=& \sigma^2, j = 1,...,k \\
		p(y_j) &=& \sum^k_{j=1} w_j \frac{1}{\sqrt{2\pi \sigma^2}} \exp\Bigg(- \frac{(y_i - \mu - (j - 1)\Delta\mu)^2}{2\sigma^2} \Bigg) \\
		\sigma^2 &\sim & \textrm{Inv-Gamma} \bigg( \frac{\nu}{2}, \frac{\zeta^2}{2} \bigg) \\
		\Delta\mu|\sigma^2 &\sim & \mathcal{N}\bigg( \eta, \frac{\sigma^2}{\kappa} \bigg) \\
		p(\theta) &\propto & \big (\sigma^2 \big)^{-\frac{\nu+3}{2}} \exp \bigg( -\frac{\zeta^2 + \kappa(\Delta\mu - \eta)^2}{2\sigma^2} \bigg) \\
		\gamma^{(m)}_{ij} &\triangleq & P(Z_i = j | y_i, \theta^{(m)}) \nonumber \\
		&=& \frac{w^{(m)}_j \phi(y_i|\mu^{(m)}_j, \sigma^{(m)})}{\sum^k_{l=1}w^{(m)}_l \phi(y_i|\mu^{(m)}_l, \sigma^{(m)})}, i=1,...,n \textrm{ and } j=1,...,k \\
		Q(\theta | \theta^{(m)}) &=& \sum^n_{i=1} \sum^k_{j=1} \gamma^{(m)}_{ij} \ln(w_j \phi(y_i | \mu + (j-1)\Delta\mu, \sigma)) \\
		\theta^{(m+1)} &=& \arg\max_{\theta} (Q(\theta | \theta^{(m)}) + \ln p(\theta))
	\end{eqnarray}

	In case of PYDT, the MAP-EM formulation of the model is as follows.
	
	\begin{eqnarray}
		p(\bold{X}, \bold{Z} | \boldsymbol{\Theta}, \mathcal{T}) &=& 
		\prod_{[uv] \in S(\mathcal{T})'} \mathcal{N}(\bold{z}_v|\bold{z}_u, \sigma^2(t_v - t_u)\bold{I}) \prod^N_{n=1} \mathcal{N}(\bold{x}_n|\bold{z}_{\textrm{pa}(n)}, \sigma^2(1 - t_{\textrm{pa}(n)})\bold{I}) \nonumber \\
		& & \\
		& & \textrm{where } \bold{Z} = \{\bold{z} \}, \boldsymbol{\Theta} = \{\bold{t}, \sigma^2\} \nonumber \\
		p(\boldsymbol{\Theta}) &=& \textrm{G}(c | a_c, b_c) \textrm{G}(1/\sigma^2 | a_{\sigma^2}, b_{\sigma^2}) \prod_{v \in \mathcal{I}} p(t_v | c, \mathcal{T}) \\
		Q(\boldsymbol{\Theta} | \boldsymbol{\Theta}') &=& \sum_{[uv] \in S(\mathcal{T})'} \Bigg(-\frac{D}{2}\ln2\pi\sigma^2(t_v - t_u) - \frac{ \mathbb{E}_{p(\bold{z}|\bold{x}, \bold{t}, \sigma^2)} \big[\parallel \bold{z}_v - \bold{z}_u \parallel^2 \big]}{2\sigma^2(t_v - t_u)} \Bigg) \nonumber \\
	 	&+& \sum^N_{n=1} \Bigg(-\frac{D}{2}\ln2\pi\sigma^2(1 - t_{\textrm{pa}(n)}) - \frac{\mathbb{E}_{p(\bold{z}|\bold{x}, \bold{t}, \sigma^2)} \big[ \parallel \bold{x}_v - \bold{z}_{\textrm{pa}(n)} \parallel^2 \big]}{2\sigma^2(1 - t_{\textrm{pa}(n)})} \Bigg) \nonumber \\
	 	& & \\
	 	\ln p(\boldsymbol{\Theta}) &=& \Bigg(a_c \ln{b_c} + (a_c - 1)\ln{c} - b_cc - \ln{\Gamma(a_c)} \nonumber \\
	 	& & + a_{\sigma^2} \ln{b_{\sigma^2}} + (a_{\sigma^2} - 1)\ln{\frac{1}{\sigma^2}} - b_{\sigma^2}\frac{1}{\sigma^2} - \ln{\Gamma(a_{\sigma^2})} \nonumber \\
	 	& & + \sum_{v \in \mathcal{I}} (cJ^{\theta, \alpha}_{\bold{n}_v} - 1) \ln{c(1 - t_v)} \Bigg)
	\end{eqnarray}
	
	However, some distributions have conjugacy. Hence, those parameters can be marginalised out and that results in the collapsed version.
	
	\begin{eqnarray}
		\int \textrm{G}(c | a_c, b_c) \prod_{v \in \mathcal{I}} p(t_v | c, \mathcal{T}) dc &=& \frac{b^{a_c}_c}{\Gamma(a_c)}(1 - t_v)^{|\mathcal{I}|} \int c^{a_c - 1 + |\mathcal{I}|} \exp \Big( - \big( b_c - \sum_{v \in \mathcal{I}} J^{\theta, \alpha}_{\bold{n}_v} \ln{(1 - t_v)} \big) c \Big) dc \nonumber \\
		&=& \frac{b^{a_c}_c}{\Gamma(a_c)}(1 - t_v)^{|\mathcal{I}|} \frac{\Gamma(a_c + |\mathcal{I}|)}{\big( b_c - \sum_{v \in \mathcal{I}} J^{\theta, \alpha}_{\bold{n}_v} \ln{(1 - t_v)} \big)^{a_c + |\mathcal{I}|}} \nonumber \\
		&=& p(t_v | a_c, b_c, \mathcal{T})
	\end{eqnarray}

	\begin{eqnarray}
		& & \int \textrm{G}(1/\sigma^2 | a_{\sigma^2}, b_{\sigma^2}) \prod_{[uv] \in S(\mathcal{T})'} \mathcal{N}(\bold{z}_v|\bold{z}_u, \sigma^2(t_v - t_u)\bold{I}) \prod^N_{n=1} \mathcal{N}(\bold{x}_n|\bold{z}_{\textrm{pa}(n)}, \sigma^2(1 - t_{\textrm{pa}(n)})\bold{I}) d(1/\sigma^2) \nonumber \\
		&=& \frac{b^{a_{\sigma^2}}_{\sigma^2}}{\Gamma(a_{\sigma^2})}  2\pi^{-\frac{D}{2}(|\mathcal{I}| + N)} \int \bigg( \frac{1}{\sigma^2} \bigg)^{a_{\sigma^2} - 1 + \frac{D}{2}(|\mathcal{I}| + N)} \exp{\Bigg( - \bigg( b_{\sigma^2} + \frac{1}{2}\sum_{[uv] \in S(\mathcal{T})} \frac{\parallel \bold{z}_v - \bold{z}_u \parallel^2}{t_v - t_u} \bigg) \frac{1}{\sigma^2} \Bigg)} \nonumber d(1/\sigma^2) \\
		&=& \frac{b^{a_{\sigma^2}}_{\sigma^2}}{\Gamma(a_{\sigma^2})}  2\pi^{-\frac{D}{2}(|\mathcal{I}| + N)} \frac{\Gamma \big( a_{\sigma^2} + \frac{D}{2}(|\mathcal{I}| + N) \big)}{\Big( b_{\sigma^2} + \frac{1}{2}\sum_{[uv] \in S(\mathcal{T})'} \frac{\parallel \bold{z}_v - \bold{z}_u \parallel^2}{t_v - t_u} + \frac{1}{2}\sum^N_{n=1} \frac{\parallel \bold{x}_n - \bold{z}_{\textrm{pa}(n)} \parallel^2}{1 - t_{\textrm{pa}(n)}} \Big)^{a_{\sigma^2} + \frac{D}{2}(|\mathcal{I}| + N)}} \nonumber \\
		&=& p(\bold{X}, \bold{Z} | \bold{t}, a_{\sigma^2}, b_{\sigma^2})
	\end{eqnarray}

	\begin{eqnarray}
		Q(\boldsymbol{\Theta} | \boldsymbol{\Theta}')
		&=&	a_{\sigma^2}\ln{b_{\sigma^2}} - \ln{\Gamma(a_{\sigma^2})}
		+ \ln{\Gamma \bigg(a_{\sigma^2} + \frac{D}{2}(|\mathcal{I}| + N) \bigg)}
		\nonumber \\
		&-& \bigg( a_{\sigma^2} + \frac{D}{2}(|\mathcal{I}| + N) \bigg)
		\Bigg< \ln{\bigg( b_{\sigma^2} + \frac{1}{2}\sum_{[uv] \in S(\mathcal{T})'} \frac{\parallel \bold{z}_v - \bold{z}_u \parallel^2}{t_v - t_u} + \frac{1}{2}\sum^N_{n=1} \frac{\parallel \bold{x}_n - \bold{z}_{\textrm{pa}(n)} \parallel^2}{1 - t_{\textrm{pa}(n)}} \bigg)} \Bigg>
		\nonumber \\
		&+& \textrm{Const.}
	\end{eqnarray}

	\begin{eqnarray}
		\ln{p(\boldsymbol{\Theta})} 
		&=& \sum_{v \in \mathcal{I}}	 \bigg(
		a_c\ln{b_c} - \ln{\Gamma(a_c)} + |\mathcal{I}|\ln{(1 - t_v)} 
		\nonumber \\
		& & + \ln{\Gamma(a_c + |\mathcal{I}|)}
		- (a_c + \mathcal{I})\ln{\Big( b_c - \sum_{v \in \mathcal{I}}J^{\theta, \alpha}_{\bold{n}_v} \ln{(1 - t_v)} \Big)} \bigg)
		\nonumber \\
		&=& |\mathcal{I}|\big( a_c\ln{b_c} - \ln{\Gamma(a_c)} + \ln{\Gamma(a_c + |\mathcal{I}|)} \big)
		\nonumber \\
		& & + \sum_{v \in \mathcal{I}} \bigg( |\mathcal{I}|\ln{(1 - t_v)} - (a_c + \mathcal{I})\ln{\Big( b_c - \sum_{v \in \mathcal{I}}J^{\theta, \alpha}_{\bold{n}_v} \ln{(1 - t_v)} \Big)} \bigg)
		\nonumber \\
	\end{eqnarray}

	\begin{eqnarray}
	\frac{\partial \big( Q(\boldsymbol{\Theta}|\boldsymbol{\Theta}') + \ln{p(\boldsymbol{\Theta})} \big)}{\partial t_v} \\
	\frac{\partial \big( Q(\boldsymbol{\Theta}|\boldsymbol{\Theta}') + \ln{p(\boldsymbol{\Theta})} \big)}{\partial a_{\sigma^2}} \\
	\frac{\partial \big( Q(\boldsymbol{\Theta}|\boldsymbol{\Theta}') + \ln{p(\boldsymbol{\Theta})} \big)}{\partial b_{\sigma^2}} \\
	\frac{\partial \big( Q(\boldsymbol{\Theta}|\boldsymbol{\Theta}') + \ln{p(\boldsymbol{\Theta})} \big)}{\partial a_c} \\
	\frac{\partial \big( Q(\boldsymbol{\Theta}|\boldsymbol{\Theta}') + \ln{p(\boldsymbol{\Theta})} \big)}{\partial b_c}
	\end{eqnarray}
	
	\subsection{Approximation of model evidence}
	Use the decomposition with variational distributions.
	
	\begin{eqnarray}
		\ln P(\bold{X}|\boldsymbol{\theta}) &=& \ln \Big\{\sum_\bold{Z}P(\bold{X}, \bold{Z} | \boldsymbol{\theta}) \Big\} \nonumber \\
		&=& \ln \Big\{\sum_\bold{Z}q(\bold{Z})\frac{P(\bold{X}, \bold{Z}|\boldsymbol{\theta})}{q(\bold{Z})} \Big\} \nonumber \\
		&=& \sum_\bold{Z}q(\bold{Z})\ln \Big\{ \frac{P(\bold{X}, \bold{Z}| \boldsymbol{\theta})}{q(\bold{Z})} \Big\} + \sum_\bold{Z}q(\bold{Z})\ln \Big\{ \frac{q(\bold{Z})}{P(\bold{Z}|\bold{X}, \boldsymbol{\theta})} \Big\}
	\end{eqnarray}
	
	$\bold{z}, c, \sigma^2$ have analytical posterior. In addition, the exact posterior of $\bold{t}$ can also be computed in a feasible way up to the error due to the finite numerical precision. Hence, we consider that the second term of (40), which is a Kullback-Leibler divergence of the calculated posterior and theoretical posterior in this context, vanishes. Incidentally, this is equivalent to calculate the ELBO of the model.
	
	\begin{eqnarray}
		& & \sum_\bold{Z}q(\bold{Z})\ln \Big\{ \frac{P(\bold{X}, \bold{Z}| \boldsymbol{\theta})}{q(\bold{Z})} \Big\} \nonumber \\
		&=& \mathbb{E}_{q(\bold{Z})} \big[ \ln{P(\bold{X}, \bold{Z}|\boldsymbol{\theta})} \big]	+ H_{q}(\bold{Z})
		\nonumber \\
		&=& a_c\ln{b_c} + (a_c - 1)\mathbb{E}[\ln{c}] - b_c\mathbb{E}[c] -\ln{\Gamma(a_c)} \nonumber \\
		& & + a_{\sigma^2}\ln{b_{\sigma^2}} + (a_{\sigma^2} - 1)\mathbb{E}\bigg[ \ln{\frac{1}{\sigma^2}} \bigg] - b_{\sigma^2}\mathbb{E}\bigg[ \frac{1}{\sigma^2} \bigg] - \ln{\Gamma(a_{\sigma^2})} \nonumber \\
		& & + \sum_{v \in \mathcal{I}} \Big\{ \big( \mathbb{E}[c]J^{\theta, \alpha}_{\bold{n}_v} - 1 \big) \mathbb{E}[\ln{c}] + \big( \mathbb{E}[c]J^{\theta, \alpha}_{\bold{n}_v} - 1 \big)\mathbb{E} \big[ \ln{(1 - t_v)} \big] \Big\} \nonumber \\
		& & + \sum_{[uv] \in S(\mathcal{T})'} \Bigg\{-\frac{D}{2} \bigg(\ln{2\pi} - \mathbb{E}\bigg[ \ln{\frac{1}{\sigma^2}} \bigg] + \mathbb{E} \big[ \ln{(t_v - t_u) \big]} \bigg) \nonumber \\
		& & - \mathbb{E}\bigg[ \frac{1}{\sigma^2} \bigg] \mathbb{E}\bigg[ \frac{1}{t_v - t_u} \bigg] \frac{ \sum^D_{d=1} \mathbb{E} [z_{v,d}^2] - 2\mathbb{E}[z_{v,d}]\mathbb{E}[z_{u,d}] + \mathbb{E}[z_{u,d}^2] \big]}{2} \Bigg\} \nonumber \\
	 	& & + \sum^N_{n=1} \Bigg\{-\frac{D}{2} \bigg( \ln{2\pi} - \mathbb{E} \Big[ \ln{\frac{1}{\sigma^2}} \Big] + \mathbb{E} \Big[ \ln(1 - t_{\textrm{pa}(n)}) \Big] \bigg) \nonumber \\
	 	& & - \mathbb{E}\bigg[ \frac{1}{\sigma^2} \bigg] \mathbb{E}\bigg[ \frac{1}{1 - t_{\textrm{pa}(n)}} \bigg] \frac{\sum^D_{d=1} x^2_{v,d} - 2x_{n, d}\mathbb{E}[z_{\textrm{pa}(n), d}] + \mathbb{E}[z^2_{\textrm{pa}(n), d}] \big]}{2} \Bigg\} \nonumber \\
	 	& & + H_q(\bold{Z})
	\end{eqnarray}
	\begin{eqnarray}
		\mathbb{E}[\ln{c}] &\approx & \ln{\mathbb{E}[c]} - \frac{\mathbb{V}[c]}{2\mathbb{E}[c]^2} \nonumber \\
		&=& \ln{ \big( a_c + |\mathcal{I}| \big)} - \ln{ \Big( b_c - \sum_{v \in \mathcal{I}}J^{\theta, \alpha}_{\bold{n}_v}\ln{(1 - t_v})\Big)} + \frac{1}{2 \big( a_c + |\mathcal{I}| \big)} \\
		\mathbb{E} \bigg[ \ln{\frac{1}{\sigma^2}} \bigg] &\approx & \ln{\mathbb{E} \bigg[ \frac{1}{\sigma^2} \bigg]} - \frac{\mathbb{V}\bigg[ \frac{1}{\sigma^2} \bigg]}{2\mathbb{E}\bigg[ \frac{1}{\sigma^2} \bigg]^2} \nonumber \\
		&=& \ln{ \big( a_{\sigma^2} + \frac{D(|\mathcal{I}| + N)}{2} \big) } - \ln{ \Big( b_{\sigma^2} + \frac{1}{2}\sum_{[uv] \in S(\mathcal{T})}\frac{\sum^D_{d=1}(z_{v,d} - z_{u,d})^2}{t_v - t_u}\Big)} \nonumber \\
		& & + \frac{1}{2 \big( a_{\sigma^2} + \frac{D(|\mathcal{I}| + N)}{2} \big)}	 \nonumber \\
		\mathbb{E} \big[ \ln{(1 - t_v)} \big] &\approx & \ln{ \big( 1 - \mathbb{E}[t_v] \big)} - \frac{\mathbb{V}[t_v]}{2 \big( 1 - \mathbb{E}[t_v] \big)^2} \\
		\mathbb{E} \big[ \ln{(t_v - t_u)} \big] &\approx & \ln{(\mathbb{E}[t_v] - \mathbb{E}[t_u])} \nonumber \\
		& & + \frac{1}{2} \Big\{ -\frac{\mathbb{V}[t_v]}{(\mathbb{E}[t_v] - \mathbb{E}[t_u])^2} + \frac{2\big( \mathbb{E}[t_v t_u] - \mathbb{E}[t_v] \mathbb{E}[t_u] \big)}{(\mathbb{E}[t_v] - \mathbb{E}[t_u])^2} - \frac{\mathbb{V}[t_u]}{(\mathbb{E}[t_v] - \mathbb{E}[t_u])^2} \Big\} \nonumber \\
		&=& \ln{(\mathbb{E}[t_v] - \mathbb{E}[t_u])} + \frac{1}{2} \Big\{ -\frac{\mathbb{V}[t_v] + \mathbb{V}[t_u] - 2\big( \mathbb{E}[t_v t_u] - \mathbb{E}[t_v] \mathbb{E}[t_u] \big)}{(\mathbb{E}[t_v] - \mathbb{E}[t_u])^2} \Big\} \\
		\mathbb{E} \bigg[\frac{1}{t_v - t_u} \bigg] &\approx & \frac{1}{\mathbb{E}[t_v] - \mathbb{E}[t_u]} + \frac{1}{2} \Big\{ \frac{2\mathbb{V}[t_v] + 2\mathbb{V}[t_u] - 4\big( \mathbb{E}[t_v t_u] - \mathbb{E}[t_v] \mathbb{E}[t_u] \big)}{ \big( \mathbb{E}[t_v] - \mathbb{E}[t_u] \big)^3} \Big\}
	\end{eqnarray}

Here, the expectations of $\bold{z}$ terms with the posterior distributions create a loopy graph. Hence, we calculate this posterior in a mean-field approximation manner. The expectation by directly substituting the samples of parent and children nodes to the calculation of each node $v$ instead of recursively taking expectations of parent and children nodes which again introduces the node $v$.
	
	Although the above equations show 2nd order approximation, we basically use 0th order approximation which is equal to 1st order approximation.
	
\end{document}